Download LLaMA model from the below drive link and save in the project root folder.
Download and extract the offline_models.zip file.
https://drive.google.com/drive/folders/1dlY_O8WX9Z0swQkWEO_1dvX-j_AbZdUL?usp=sharing


```bash
pip install streamlit
```

(For `requirements.txt`, run:)

```bash
pip install -r requirements.txt
```

---
##  Running the Application

Inside the project directory, run:

```bash
streamlit run main.py
```

If the path is absolute (like your image):

```bash
streamlit run E:\CPP\my_fraud_project\main.py
```

---

##  Application Access

Once started, Streamlit will show:

```
Local URL: http://localhost:8501
Network URL: http://<your-ip>:8501
```

